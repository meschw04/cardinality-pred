{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Overcoming SOTA Performance On IMDB With JOB-Light\n",
    "*By Marcus Schwarting and Andronicus Samsundar Rajasukumar*\n",
    "\n",
    "In this notebook, we will:\n",
    "- Introduce the Kipf et. al. model that we wish to improve upon\n",
    "- Show various implementations of featurization routines, and discuss their pros and cons\n",
    "- Discuss changes to the Kipf implementation that yielded overall improvements in accuracy and training time\n",
    "\n",
    "The performance benchmark that we wish to beat, as described in the literature on the JOB-light test query set on the IMDB dataset, is as follows:\n",
    "\n",
    "| Metric | Value |\n",
    "| ---- | ---- |\n",
    "|Median | 3.82|\n",
    "|90th Percentile| 78.4|\n",
    "|95th Percentile|362|\n",
    "|Max|1110|\n",
    "|Mean|57.9|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFIED VERSION OF KIPF ET AL CODE (originally from https://github.com/andreaskipf/learnedcardinalities)#\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mscn.util import *\n",
    "from mscn.data import get_train_datasets, load_data, make_dataset\n",
    "from mscn.model import SetConv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Kipf MSCN Model\n",
    "The authors achieve the above benchmark performance by using a multi-set convolutional network (MSCN). We have re-implemented their methods with some changes that have marginally improved on the state of the art. Below we re-use some of their code infrastructure and point out important changes where they are applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_torch(vals, min_val, max_val):\n",
    "    #Read from \"imdb_max_min.csv\"\n",
    "    vals = (vals * (max_val - min_val)) + min_val\n",
    "    return torch.exp(vals)\n",
    "\n",
    "\n",
    "def qerror_loss(preds, targets, min_val, max_val):\n",
    "    #Returns Q-error, can also return MAE as desired.\n",
    "    qerror = []\n",
    "    preds = unnormalize_torch(preds, min_val, max_val)\n",
    "    targets = unnormalize_torch(targets, min_val, max_val)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i] / targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i] / preds[i])\n",
    "    return torch.mean(torch.cat(qerror))\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    #The workhorse. Evaluates the final model and runs predictions.\n",
    "    preds = []\n",
    "    t_total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "\n",
    "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "        t = time.time()\n",
    "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "        t_total += time.time() - t\n",
    "\n",
    "        for i in range(outputs.data.shape[0]):\n",
    "            preds.append(outputs.data[i])\n",
    "\n",
    "    return preds, t_total\n",
    "\n",
    "def print_qerror(preds_unnorm, labels_unnorm):\n",
    "    qerror = []\n",
    "    for i in range(len(preds_unnorm)):\n",
    "        if preds_unnorm[i] > float(labels_unnorm[i]):\n",
    "            qerror.append(preds_unnorm[i] / float(labels_unnorm[i]))\n",
    "        else:\n",
    "            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i]))\n",
    "\n",
    "    print(f\"Median: {np.median(qerror)}\")\n",
    "    print(f\"90th percentile: {np.percentile(qerror, 90)}\")\n",
    "    print(f\"95th percentile: {np.percentile(qerror, 95)}\")\n",
    "    print(f\"99th percentile: {np.percentile(qerror, 99)}\")\n",
    "    print(f\"Max: {np.max(qerror)}\")\n",
    "    print(f\"Mean: {np.mean(qerror)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(workload_name, num_queries=1000, num_epochs=100, \\\n",
    "                      batch_size=100, hid_units=256, verbose=False,write=False):\n",
    "    # Load training and validation data\n",
    "    num_materialized_samples = 1000\n",
    "    dicts, column_min_max_vals, min_val, max_val, labels_train, \\\n",
    "    labels_test, max_num_joins, max_num_predicates, \\\n",
    "    train_data, test_data = get_train_datasets('all_train_queries.sql', num_queries, \\\n",
    "                                               num_materialized_samples)\n",
    "    table2vec, column2vec, op2vec, join2vec = dicts\n",
    "\n",
    "    # Train model\n",
    "    sample_feats = len(table2vec) + num_materialized_samples\n",
    "    predicate_feats = len(column2vec) + len(op2vec) + 1\n",
    "    join_feats = len(join2vec)\n",
    "\n",
    "    model = SetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005) #lr=0.001 originally\n",
    "    \n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_total = 0.\n",
    "\n",
    "        for batch_idx, data_batch in enumerate(train_data_loader):\n",
    "\n",
    "            samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "            loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if verbose:\n",
    "            print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
    "\n",
    "    # Get final training and validation set predictions\n",
    "    preds_train, t_total = predict(model, train_data_loader)\n",
    "    if verbose:\n",
    "        print(\"Prediction time per training sample: {}\".format(t_total / len(labels_train) * 1000))\n",
    "\n",
    "    preds_test, t_total = predict(model, test_data_loader)\n",
    "    if verbose:\n",
    "        print(\"Prediction time per validation sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "    # Unnormalize\n",
    "    preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
    "    labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
    "\n",
    "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "    labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
    "\n",
    "    # Print metrics\n",
    "    if verbose:\n",
    "        print(\"\\nQ-Error training set:\")\n",
    "        print_qerror(preds_train_unnorm, labels_train_unnorm)\n",
    "        print(\"\\nQ-Error validation set:\")\n",
    "        print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
    "        print(\"\")\n",
    "\n",
    "    # Load test data\n",
    "    file_name = \"workloads/\" + workload_name\n",
    "    joins, predicates, tables, samples, label = load_data(file_name, num_materialized_samples)\n",
    "\n",
    "    # Get feature encoding and proper normalization\n",
    "    samples_test = encode_samples(tables, samples, table2vec)\n",
    "    predicates_test, joins_test = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
    "    labels_test, _, _ = normalize_labels(label, min_val, max_val)\n",
    "    if verbose:\n",
    "        print(f\"Number of test samples: {len(labels_test)}\")\n",
    "\n",
    "    max_num_predicates = max([len(p) for p in predicates_test])\n",
    "    max_num_joins = max([len(j) for j in joins_test])\n",
    "\n",
    "    # Get test set predictions\n",
    "    test_data = make_dataset(samples_test, predicates_test, joins_test, labels_test, max_num_joins, max_num_predicates)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    preds_test, t_total = predict(model, test_data_loader)\n",
    "    if verbose:\n",
    "        print(f\"Prediction time per test sample: {t_total / len(labels_test) * 1000}\")\n",
    "\n",
    "    # Unnormalize\n",
    "    preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nQ-Error, {workload_name}:\")\n",
    "    print_qerror(preds_test_unnorm, label)\n",
    "\n",
    "    # Write predictions\n",
    "    if write:\n",
    "        file_name = f\"results/predictions_{workload_name}.csv\"\n",
    "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "        with open(file_name, \"w\") as f:\n",
    "            for i in range(len(preds_test_unnorm)):\n",
    "                f.write(f'{preds_test_unnorm[i]},{label[i]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (recreated and retrained) MSCN from Kipf et. al.:\n",
      "\n",
      "Q-Error, job-light:\n",
      "Median: 3.829080001743435\n",
      "90th percentile: 79.58870873669316\n",
      "95th percentile: 381.1589145561346\n",
      "99th percentile: 937.5885201549474\n",
      "Max: 1271.7475329481463\n",
      "Mean: 44.07001456032248\n",
      "Total Time: 217.4167 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Original (recreated and retrained) MSCN from Kipf et. al.:\\n')\n",
    "start_time = time.time()\n",
    "train_and_predict(testset='job-light', num_queries=5000, epochs=1000, batch_size=100, hid=256)\n",
    "print(f'Total Time: {round((time.time()-start_time),4)} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Data Encoding\n",
    "Below shows the difference between the original MSCN implementation of predicate data encoding versus our featurized predicate encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THE ORIGINAL CODE IS AVAILABLE FROM KIPF ET AL, mscn/utils.py ####\n",
    "def encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec):\n",
    "    predicates_enc = []\n",
    "    joins_enc = []\n",
    "    for i, query in enumerate(predicates):\n",
    "        predicates_enc.append(list())\n",
    "        joins_enc.append(list())\n",
    "        for predicate in query:\n",
    "            if len(predicate) == 3:\n",
    "                # Proper predicate\n",
    "                column = predicate[0]\n",
    "                operator = predicate[1]\n",
    "                val = predicate[2]\n",
    "                norm_val = normalize_data(val, column, column_min_max_vals)\n",
    "\n",
    "                pred_vec = []\n",
    "                pred_vec.append(column2vec[column])\n",
    "                pred_vec.append(op2vec[operator])\n",
    "                pred_vec.append(norm_val)\n",
    "                pred_vec = np.hstack(pred_vec)\n",
    "            else:\n",
    "                pred_vec = np.zeros((len(column2vec) + len(op2vec) + 1))\n",
    "            predicates_enc[i].append(pred_vec)\n",
    "            predicates_enc[i] = predicates_enc[i].flatten()\n",
    "        \n",
    "        for predicate in joins[i]:\n",
    "            # Join instruction\n",
    "            join_vec = join2vec[predicate]\n",
    "            joins_enc[i].append(join_vec)\n",
    "    return predicates_enc, joins_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OUR UPDATES TO KIPF ET AL DATA ENCODING SCHEMA ####\n",
    "def encode_data_NEW(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec):\n",
    "    predicates_enc = []\n",
    "    joins_enc = []\n",
    "    for i, query in enumerate(predicates):\n",
    "        predicates_enc.append(list())\n",
    "        joins_enc.append(list())\n",
    "        for predicate in query:\n",
    "            column = predicate[0]\n",
    "            operator = predicate[1]\n",
    "            val = predicate[2]\n",
    "            norm_val = normalize_data(val, column, column_min_max_vals)\n",
    "            #MAJOR FEATURIZATION CHANGES HERE\n",
    "            col_onehot = column2vec[column]\n",
    "            oper_onehot = op2vec[operator]\n",
    "            pred_vec = np.zeros(len(col_onehot)*len(oper_onehot))\n",
    "            for j in range(len(col_onehot)):\n",
    "                if col_onehot==1:\n",
    "                    pred_vec[3*j:3*j+3]=oper_onehot*norm_val\n",
    "\n",
    "        predicates_enc[i].append(pred_vec)\n",
    "\n",
    "        for predicate in joins[i]:\n",
    "            # Join instruction\n",
    "            join_vec = join2vec[predicate]\n",
    "            joins_enc[i].append(join_vec)\n",
    "    return predicates_enc, joins_enc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicate Encoding Scheme Comparison\n",
    "Our main insight on improving the featurization is as follows. Suppose I have a query with the following predicates:\n",
    "$$(b<0.5) \\wedge (d>0.2) \\wedge (e=0.3)$$\n",
    "on some set of predicates $\\{a,b,c,d,e\\}$ where we assume each normalized attribute ranges between $[0,1]$.\n",
    "Assuming an upward limit of four predicates, the Kipf implementation would featurize this predicate set as follows:\n",
    "\n",
    "```(Predicate on a)\n",
    "[0 1 0 0 0 1 0 0 0.5]\n",
    " a b c d e < > = val\n",
    " -- AND --\n",
    "(Predicate on d)\n",
    "[0 0 0 1 0 0 1 0 0.2]\n",
    " a b c d e < > = val\n",
    " -- AND --\n",
    "(Predicate on e)\n",
    "[0 0 0 0 1 0 0 1 0.3]\n",
    " a b c d e < > = val\n",
    "FINAL REPRESENTATION (assuming a four predicate maximum):\n",
    "[0 1 0 0 0 1 0 0 0.5 0 0 0 1 0 0 1 0 0.2 0 0 0 0 1 0 0 1 0.3 0 0 0 0 0 0 0 0 0]\n",
    "Final Length of Predicate Featurization: 36\n",
    "(average of 7.2 values per table attribute)```\n",
    "\n",
    "By contrast, we choose to featurize this predicate set as follows:\n",
    "\n",
    "```[0 0 0   0.5 0 0   0 0 0     0 0.2 0    0 0 0.3 ]  Final Featurization\n",
    " < > =    <  > =   < > =      <  >  =    < >  =    Equality Operators\n",
    "   a        b        c          d          e       Variables\n",
    "FINAL REPRESENTATION:\n",
    "[0 0 0 0.5 0 0 0 0 0 0 0.2 0 0 0 0.3]\n",
    "Final Length of Predicate Featurization: 15\n",
    "(constant average of 3 values per table attribute)```\n",
    "\n",
    "There are a number of benefits to this featurization. First, there is no upward limit on the number of predicates that can be placed in a query. The predicate featurization length has no dependence on the number of predicates in a query. There is also no order dependence; that is, presumably\n",
    "\n",
    "\n",
    "$$[\\color{green}{\\text{0, 1, 0, 0, 0, 1, 0, 0, 0.5,}} \\color{red}{\\text{0, 0, 0, 1, 0, 0, 1, 0, 0.2,}}   0, 0, 0, 0, 1, 0, 0, 1, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0]$$\n",
    "\n",
    "and \n",
    "\n",
    "$$[ \\color{red}{\\text{0, 0, 0, 1, 0, 0, 1, 0, 0.2,}} \\color{green}{\\text{0, 1, 0, 0, 0, 1, 0, 0, 0.5,}}  0, 0, 0, 0, 1, 0, 0, 1, 0.3, 0, 0, 0, 0, 0, 0, 0, 0, 0]$$\n",
    "\n",
    "should map to an identical cardinality, and indeed be identical queries (we have merely switched the order of predicate operations), but have very different predicate featurized representations. It would appear that the MSCN is not flexible enough to recognize this difference. Even when aggregating over sets of predicates (as the MSCN can be adjusted to do), the improved predicate featurization still out-performs the previous implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained MSCN Architecture with Updated Featurization:\n",
      "\n",
      "Q-Error job-light:\n",
      "Median: 3.3707934686744982\n",
      "90th percentile: 44.26868661918655\n",
      "95th percentile: 197.39683996127513\n",
      "99th percentile: 782.6566606666486\n",
      "Max: 954.0733123971569\n",
      "Mean: 41.41337581462835\n",
      "Total Time: 210.5493 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Retrained MSCN Architecture with Updated Featurization:\\n')\n",
    "start_time = time.time()\n",
    "#Note: We use this altered function on the back end with other utis, and integrate accordingly.\n",
    "train_and_predict_NEW(testset='job-light', num_queries=5000, epochs=1000, batch_size=100, hid=256)\n",
    "print(f'Total Time: {round((time.time()-start_time),4)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Performance on JOB-Light\n",
    "| Q-Error Metric (JOB-Light) | Kipf et. al. Results | Improved Featurization | Cardinality Sampling |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "|Median | 3.82| __3.37__ | 4.55 |\n",
    "|90th Percentile| 78.4| __44.3__ | 76.3 |\n",
    "|95th Percentile|362| __197__ | 302 |\n",
    "|Max|1110| __954__ | 271841 |\n",
    "|Mean|57.9| __41.4__ | 4872.7 |\n",
    "|Training Time on GPU (min)|3.6|3.5| __2.8__ |\n",
    "\n",
    "For the cardinality sampling model, we also have a number of trials with different pre-sets to optimize performance. We have some indication that this method can surpass state-of-the-art, however we also notice a \"long tail\" of error at higher percentiles. More training queries may alleviate this, however this information is costly to generate.\n",
    "\n",
    "| JOB-light       |M/A Trial 1|M/A Trial 2|M/A Trial 3|M/A Trial 4| MSCN (Benchmark) |\n",
    "|-----------------|:-----------:|:-----------:|:-----------:|:-----------:|-------|\n",
    "| Samples         |        5000 |        5000 |       10000 |       10000 | 10000 |\n",
    "| Queries         |        1000 |        4400 |        1000 |        4400 | 90000 |\n",
    "| Median          | 7.51 | 5.23 | 4.55 | 5.07 |  __3.82__ |\n",
    "| 90th percentile | 255.5 | 215.6 | __76.3__ | 128.7 |  78.4 |\n",
    "| 95th percentile | 607 | 397 | __303__ | 279.70 |   362 |\n",
    "| 99th percentile | 67952 | 2062 | 122114 | 1333 |   __927__ |\n",
    "| Max             | 151023 | 2609 |    271841 | 1931 |  __1110__ |\n",
    "| Mean            | 2758.1 | 117.2 | 4872.7 | 78.8 |  __57.9__ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
